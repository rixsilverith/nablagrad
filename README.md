[![License](https://img.shields.io/github/license/rixsilverith/nablagrad)](https://mit-license.org/)
![Cpp Version](https://img.shields.io/badge/C%2B%2B-17-blue)

# nablagrad

Automatic differentiation is a technique to automatically compute and evaluate the partial derivatives (and therefore
gradient) of a given mathematical function.

*nablagrad* is yet another automatic differentiation library written in C++ as a proof of concept. Despite its main purpose being
learning how autodiff works and how it may be implemented, it should be a fast and reliable tool to perform first-order
differentiation.

> **Note** As of now, *nablagrad* only supports forward-mode autodiff, but it is expected to also provide
support for reverse-mode autodiff at some point.

---

## License

*nablagrad* is licensed under the MIT License. See [LICENSE](LICENSE) for more information. A copy of the license can be found along with the code.

---

## References

- Baydin, A. G.; Pearlmutter, B. A.; Radul, A. A.; Siskind, J. M. (2018). *Automatic differentiation in machine learning: a survey*. [https://arxiv.org/abs/1502.05767](https://arxiv.org/abs/1502.05767)
